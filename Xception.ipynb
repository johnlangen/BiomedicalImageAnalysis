{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Il1DbpKDEMZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set paths\n",
        "biomed_path = \"/content/drive/MyDrive/BIOMED\"\n",
        "ham10000_path = os.path.join(biomed_path, \"HAM10000\")\n",
        "\n",
        "# Load metadata\n",
        "metadata = pd.read_csv(os.path.join(biomed_path, \"HAM10000_metadata.csv\"))\n",
        "\n",
        "# Update the image_id column to include the full path to the images\n",
        "metadata[\"image_id\"] = metadata.apply(lambda row: os.path.join(biomed_path, \"HAM10000_images_part_1\", row[\"image_id\"] + \".jpg\") if row[\"image_id\"][0] == 'I' else os.path.join(biomed_path, \"HAM10000_images_part_2\", row[\"image_id\"] + \".jpg\"), axis=1)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_df, test_df = train_test_split(metadata, test_size=0.2, random_state=42, stratify=metadata[\"dx\"])\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=42, stratify=train_df[\"dx\"])\n",
        "\n",
        "# Data preprocessing and augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.1,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "image_size = 299\n",
        "batch_size = 32\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    train_df,\n",
        "    x_col=\"image_id\",\n",
        "    y_col=\"dx\",\n",
        "    target_size=(image_size, image_size),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_dataframe(\n",
        "    val_df,\n",
        "    x_col=\"image_id\",\n",
        "    y_col=\"dx\",\n",
        "    target_size=(image_size, image_size),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        ")\n",
        "\n",
        "# Load Xception model\n",
        "base_model = Xception(weights=\"imagenet\", include_top=False, input_shape=(image_size, image_size, 3))\n",
        "\n",
        "# Add custom layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "output = Dense(7, activation=\"softmax\", kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 25\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.n // batch_size,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.n // batch_size,\n",
        "    epochs=num_epochs,\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"xception_ham10000.h5\")\n"
      ]
    }
  ]
}